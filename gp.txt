Regression, the Gaussian Process way
Gaussian process regression is a non-linear regression model. Gaussian process models are used extensively in areas like hyperparameter tuning and image processing. Much like the well-known linear regression or neural network models, it is at its core, a function approximator which fits a function to a set of points. 
But, unlike linear regression or neural networks, it is a non-parametric model, which means that there are no explicit parameters like weights or coefficients and whenever a prediction has to be made, we need the entire training set. Although, this could be viewed as a disadvantage, there are subtle ways we can get around this issue through approximation techniques.
It is also a bayesian model, which means you can impart prior knowledge directly into the model to improve its accuracy and more importantly, you can get an estimate of uncertainty of your predictions. For example, if I am predicting housing prices in a region based on features like number of floors, and I am aware that the housing prices are around 100,000$, I could simply set the mean of my Gaussian prior to be 100,000 and the model's accuracy would be much better on the test dataset. How important is it to know the variance? Let's say I have enough data for houses which have 2 or more floors but not enough for 1, I would expect my model to not be so confident with its prediction by having a high variance for single floor houses.
Can't we just fit a function using Maximum Likelihood Estimate (MLE) ? Yes, we could, but unlike GP, you can't impose prior knowledge into it and the standard MLE estimate for a Gaussian model assumes that it has a constant variance. Although, you could estimate the variance using MLE, you are still stuck with something thats constant even in places where you shouldn't be so confident.